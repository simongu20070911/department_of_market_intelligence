# /department_of_market_intelligence/tools/mock_llm_agent.py
"""Mock LLM agent for dry run mode - simulates agent behavior without API calls."""

import asyncio
from typing import AsyncGenerator
from google.adk.agents import BaseAgent
from google.adk.agents.invocation_context import InvocationContext
from google.adk.events import Event, EventActions
from .. import config

class MockLlmAgent(BaseAgent):
    """Mock LLM agent that simulates responses without making API calls."""
    
    def __init__(self, name: str, instruction: str = "", **kwargs):
        super().__init__(name=name, **kwargs)
        self._instruction = instruction  # Use private attribute to avoid Pydantic validation
        
    async def _simulate_llm_thinking(self):
        """Simulate realistic LLM thinking and response patterns."""
        import random
        
        thinking_phrases = [
            "Let me analyze the current task and context...",
            "I need to carefully review the requirements...", 
            "Examining the session state to understand what's needed...",
            "Reviewing the instructions and available information...",
            "Let me think through this step by step..."
        ]
        
        print(f"ü§î [{self.name}]: {random.choice(thinking_phrases)}")
        
    async def _run_async_impl(self, ctx: InvocationContext) -> AsyncGenerator[Event, None]:
        print(f"\nü§ñ [{self.name}]: Starting work...")
        print(f"[DRY RUN] Session state keys: {list(ctx.session.state.keys())}")
        
        # Simulate realistic LLM conversation responses
        await self._simulate_llm_thinking()
        
        # Simulate different agent behaviors based on name
        if "Validator" in self.name:
            # Actually create critique files during dry run
            import os
            
            # Get the task-specific outputs directory with proper structure
            task_id = ctx.session.state.get('task_id', 'test_research_session')
            base_dir = f"/home/gaen/agents_gaen/department_of_market_intelligence/outputs/{task_id}"
            planning_dir = f"{base_dir}/planning"
            os.makedirs(planning_dir, exist_ok=True)
            
            if "Junior" in self.name:
                # Simulate LLM conversation
                print(f"üí≠ [{self.name}]: I'll review this research plan thoroughly. Let me examine each section...")
                print(f"üìù [{self.name}]: I can see the plan has good structure, but I've identified several areas that need improvement.")
                print(f"üîç [{self.name}]: Writing detailed critique with specific recommendations...")
                
                # Create junior critique file in planning directory
                critique_path = f"{planning_dir}/critique_junior_v{ctx.session.state.get('plan_version', 0)}.md"
                critique_content = f"""# Junior Validator Critique - {task_id}

## Review Summary
[DRY RUN] The research plan shows solid foundation but could benefit from enhancements:

### Strengths Identified
‚úÖ Clear research objectives and scope
‚úÖ Appropriate statistical methodology outlined
‚úÖ Reasonable timeline and resource allocation
‚úÖ Relevant data sources identified

### Areas for Improvement

#### 1. Statistical Rigor
- **Recommendation**: Add power analysis for sample size justification
- **Details**: Current plan lacks statistical power calculations for detecting meaningful effect sizes
- **Impact**: May lead to underpowered studies or inflated sample sizes

#### 2. Data Quality Controls
- **Recommendation**: Expand data cleaning and validation procedures
- **Details**: Include outlier detection methods, missing data imputation strategies
- **Impact**: Ensures robust findings and reproducible results

#### 3. Risk Mitigation
- **Recommendation**: Add contingency plans for data availability issues
- **Details**: Specify alternative data sources and fallback methodologies
- **Impact**: Prevents project delays due to unforeseen data constraints

#### 4. Regulatory Context
- **Recommendation**: Include recent regulatory changes in analysis framework
- **Details**: Account for rule changes that may confound time series analysis
- **Impact**: Improves causal interpretation of findings

## Overall Assessment
The plan demonstrates strong research fundamentals but requires methodological refinements before proceeding to implementation.

**Status**: Requires revision
**Priority**: Medium - addressable with focused improvements

---
*Generated by Junior Validator during dry run*
*Task ID: {task_id}*
*Plan Version: {ctx.session.state.get('plan_version', 0)}*
"""
                with open(critique_path, 'w') as f:
                    f.write(critique_content)
                print(f"‚úÖ [{self.name}]: Created critique file: {critique_path}")
                print(f"üìã [{self.name}]: I've completed my review. The plan needs some refinements, but it's on the right track!")
                
                ctx.session.state['junior_critique_artifact'] = critique_path
                
            elif "Senior" in self.name:
                # Simulate LLM conversation for Senior Validator
                print(f"üéì [{self.name}]: Conducting final validation review...")
                
                # Create senior critique file in planning directory
                critique_path = f"{planning_dir}/critique_senior_v{ctx.session.state.get('plan_version', 0)}.md"
                
                # Check if we're validating an executor failure
                current_execution_status = ctx.session.state.get('execution_status', '')
                if current_execution_status == 'critical_error':
                    print(f"‚ö†Ô∏è [{self.name}]: CRITICAL ERROR DETECTED! This requires immediate escalation.")
                    critique_content = f"""# Senior Validator Critical Review - {task_id}

## CRITICAL ERROR DETECTED

### Error Analysis
The implementation has encountered a critical error that requires immediate attention and replanning.

**Error Type**: {ctx.session.state.get('error_type', 'Unknown')}
**Error Details**: {ctx.session.state.get('error_details', 'Not specified')}

### Root Cause Assessment
Critical failures typically stem from:
1. Insufficient error handling in implementation
2. Data availability issues not anticipated in planning
3. Resource constraints exceeding estimates
4. Methodological assumptions violated by real data

### Required Actions
‚ùå **ESCALATION REQUIRED** - Implementation must be replanned
‚ùå **FULL REVIEW** - Research plan needs comprehensive revision
‚ùå **RISK ASSESSMENT** - Additional contingencies required

**Status**: CRITICAL_ERROR - Requires replanning
**Next Steps**: Return to Orchestrator for revised implementation approach

---
*Critical review by Senior Validator*
*Task ID: {task_id}*
"""
                    ctx.session.state['validation_status'] = 'critical_error'  # Escalate for replanning
                    print(f"‚ùå [{self.name}]: Setting validation status to CRITICAL_ERROR. This needs replanning!")
                else:
                    print(f"‚úÖ [{self.name}]: Excellent! This research plan meets all our quality standards.")
                    print(f"üèÜ [{self.name}]: I'm approving this for implementation. Great work by the team!")
                    critique_content = f"""# Senior Validator Final Review - {task_id}

## APPROVAL GRANTED

### Comprehensive Assessment
After thorough review of the research plan and junior validator feedback, I approve this plan for implementation.

### Strengths Confirmed
‚úÖ **Methodological Soundness**: Statistical approaches are appropriate and well-justified
‚úÖ **Resource Allocation**: Timeline and budget estimates are realistic and achievable
‚úÖ **Data Strategy**: Multi-source approach provides robustness and validation opportunities
‚úÖ **Risk Management**: Adequate contingencies identified for major implementation risks

### Quality Assurance Checklist
- [x] Research objectives clearly defined and measurable
- [x] Statistical methodology appropriate for research questions
- [x] Data sources verified and accessible
- [x] Timeline realistic with appropriate buffers
- [x] Deliverables aligned with stakeholder expectations
- [x] Risk mitigation strategies documented
- [x] Junior validator concerns addressed

### Implementation Readiness
The research plan meets all standards for:
- **Scientific rigor**: Appropriate controls and statistical methods
- **Feasibility**: Realistic scope given available resources
- **Impact**: Clear value proposition and actionable outcomes
- **Compliance**: Adheres to institutional research standards

### Final Recommendation
**APPROVED** - Proceed to implementation phase with confidence.

The plan demonstrates exceptional preparation and should yield high-quality, actionable research findings.

---
*Final approval by Senior Validator*
*Task ID: {task_id}*
*Plan Version: {ctx.session.state.get('plan_version', 0)}*
*Approval Date: {ctx.session.state.get('current_datetime', 'unknown')}*
"""
                    ctx.session.state['validation_status'] = 'approved'  # Normal approval
                    print(f"[DRY RUN] {self.name} approved with validation_status='approved'")
                
                with open(critique_path, 'w') as f:
                    f.write(critique_content)
                print(f"[DRY RUN] ‚úÖ Created: {critique_path}")
                
                ctx.session.state['senior_critique_artifact'] = critique_path
            elif "StatisticalValidator" in self.name:
                print(f"[DRY RUN] {self.name} would validate statistical rigor:")
                print(f"[DRY RUN]   - Sample size adequacy and power analysis")
                print(f"[DRY RUN]   - Hypothesis testing methodology")
                print(f"[DRY RUN]   - Multiple testing corrections")
                print(f"[DRY RUN]   - Risk-adjusted performance metrics")
                print(f"[DRY RUN] {self.name} found no critical statistical issues")
            elif "DataHygieneValidator" in self.name:
                print(f"[DRY RUN] {self.name} would validate data hygiene:")
                print(f"[DRY RUN]   - Data cleaning procedures")
                print(f"[DRY RUN]   - Missing data handling strategies")
                print(f"[DRY RUN]   - Outlier detection and treatment")
                print(f"[DRY RUN]   - Survivorship bias prevention")
                print(f"[DRY RUN] {self.name} found no critical data hygiene issues")
            elif "MethodologyValidator" in self.name:
                print(f"[DRY RUN] {self.name} would validate research methodology:")
                print(f"[DRY RUN]   - Experimental design validity")
                print(f"[DRY RUN]   - Factor model specification")
                print(f"[DRY RUN]   - Benchmark selection rationale")
                print(f"[DRY RUN]   - Transaction cost modeling")
                print(f"[DRY RUN] {self.name} found no critical methodology issues")
            elif "EfficiencyValidator" in self.name:
                print(f"[DRY RUN] {self.name} would validate efficiency and parallelization:")
                print(f"[DRY RUN]   - Checking for missed parallelization opportunities")
                print(f"[DRY RUN]   - Validating interface contracts and stitching points")
                print(f"[DRY RUN]   - Analyzing resource utilization")
                print(f"[DRY RUN]   - Identifying sequential bottlenecks")
                print(f"[DRY RUN] {self.name} found proper parallelization with clear stitching points")
            elif "GeneralValidator" in self.name:
                print(f"[DRY RUN] {self.name} would validate general aspects:")
                print(f"[DRY RUN]   - Logical consistency and flow")
                print(f"[DRY RUN]   - Completeness of analysis")
                print(f"[DRY RUN]   - Implementation feasibility")
                print(f"[DRY RUN]   - Timeline realism")
                print(f"[DRY RUN] {self.name} found no critical general issues")
                
        elif "Chief_Researcher" in self.name:
            # Simulate LLM conversation for Chief Researcher
            import os
            
            # Get the task-specific outputs directory with proper structure
            task_id = ctx.session.state.get('task_id', 'test_research_session')
            base_dir = f"/home/gaen/agents_gaen/department_of_market_intelligence/outputs/{task_id}"
            
            # Create organized directory structure
            planning_dir = f"{base_dir}/planning"
            workspace_dir = f"{base_dir}/workspace"
            data_dir = f"{base_dir}/data"
            results_dir = f"{base_dir}/results"
            deliverables_dir = f"{base_dir}/deliverables"
            
            # Ensure all directories exist
            os.makedirs(planning_dir, exist_ok=True)
            os.makedirs(f"{workspace_dir}/src", exist_ok=True)
            os.makedirs(f"{workspace_dir}/notebooks", exist_ok=True)
            os.makedirs(f"{workspace_dir}/scripts", exist_ok=True)
            os.makedirs(f"{workspace_dir}/tests", exist_ok=True)
            os.makedirs(f"{data_dir}/raw", exist_ok=True)
            os.makedirs(f"{data_dir}/processed", exist_ok=True)
            os.makedirs(f"{data_dir}/external", exist_ok=True)
            os.makedirs(f"{results_dir}/charts", exist_ok=True)
            os.makedirs(f"{deliverables_dir}/presentations", exist_ok=True)
            
            current_task = ctx.session.state.get('current_task', '')
            if current_task == 'generate_initial_plan':
                print(f"üìä [{self.name}]: I need to create a comprehensive research plan for market microstructure analysis.")
                print(f"üìñ [{self.name}]: First, let me read the research task description to understand the requirements.")
                print(f"‚úçÔ∏è [{self.name}]: Now I'll draft a detailed plan with methodology, experiments, and deliverables...")
                # Create actual research plan file in planning directory
                plan_path = f"{planning_dir}/research_plan_v0.md"
                plan_content = f"""# Research Plan - {task_id}

## Objective
[DRY RUN] Analyze market microstructure evolution from 2020-2024, focusing on:
- Zero-commission trading impact
- Payment for order flow (PFOF) analysis  
- Rise of retail trading platforms

## Methodology
1. **Data Collection**: Multi-source approach using Yahoo Finance, FRED, SEC EDGAR
2. **Statistical Analysis**: Significance testing with multiple comparison corrections
3. **Trend Analysis**: Time series analysis of bid-ask spreads and market depth
4. **Behavioral Analysis**: Gamification impact on retail trading patterns

## Key Experiments
- Bid-ask spread analysis across asset classes (2020-2024)
- Market depth regression analysis
- Retail vs institutional trading volume comparison
- Regulatory impact assessment

## Expected Deliverables
- Quantitative analysis with statistical significance testing
- Data visualizations of key trends
- Policy recommendations based on empirical findings

## Timeline
- Data collection: 2 weeks
- Analysis: 3 weeks  
- Reporting: 1 week

Generated during dry run at: {ctx.session.state.get('current_datetime', 'unknown')}
Task ID: {task_id}
"""
                with open(plan_path, 'w') as f:
                    f.write(plan_content)
                print(f"‚úÖ [{self.name}]: Research plan created successfully: {plan_path}")
                print(f"üìã [{self.name}]: Plan includes methodology, experiments, and timeline. Ready for validation!")
                
                # Update session state
                ctx.session.state['plan_artifact_name'] = 'outputs/research_plan_v0.md'
                ctx.session.state['plan_version'] = 0
                ctx.session.state['artifact_to_validate'] = plan_path
                
            elif current_task == 'refine_plan':
                version = ctx.session.state.get('plan_version', 0) + 1
                plan_path = f"{planning_dir}/research_plan_v{version}.md"
                
                # Create refined plan
                plan_content = f"""# Research Plan v{version} - {task_id}

## Objective (Refined)
[DRY RUN] Enhanced analysis of market microstructure evolution from 2020-2024.

### Refined Focus Areas:
- **Zero-commission trading**: Quantitative impact on spreads and liquidity
- **PFOF analysis**: Revenue model effects on execution quality
- **Retail platform growth**: User behavior and market impact analysis
- **Regulatory responses**: Effectiveness of new rules and guidelines

## Enhanced Methodology
1. **Expanded Data Sources**: 
   - Primary: Yahoo Finance, FRED, SEC EDGAR
   - Secondary: FINRA data, exchange direct feeds
   - Alternative: Social media sentiment, app usage metrics

2. **Advanced Statistical Methods**:
   - Panel regression with fixed effects
   - Difference-in-differences analysis
   - Machine learning for pattern detection
   - Robustness testing with multiple model specifications

## Refined Experiments
- **Experiment 1**: Bid-ask spread evolution with volume controls
- **Experiment 2**: Market depth analysis by asset class and time
- **Experiment 3**: Retail flow impact on price discovery
- **Experiment 4**: Regulatory announcement event studies

## Risk Mitigation
- Data availability contingencies
- Multiple data provider fallbacks
- Sensitivity analysis for key assumptions

Generated during dry run refinement at: {ctx.session.state.get('current_datetime', 'unknown')}
Version: {version}
Task ID: {task_id}
"""
                with open(plan_path, 'w') as f:
                    f.write(plan_content)
                print(f"[DRY RUN] ‚úÖ Created: {plan_path}")
                
                ctx.session.state['plan_artifact_name'] = f'outputs/research_plan_v{version}.md'
                ctx.session.state['plan_version'] = version
                ctx.session.state['artifact_to_validate'] = plan_path
                print(f"[DRY RUN] {self.name} refined plan to version {version}")
                
            elif current_task == 'generate_final_report':
                # Create final report in deliverables directory
                report_path = f"{deliverables_dir}/final_report.md"
                report_content = f"""# Final Research Report - {task_id}

## Executive Summary
[DRY RUN] This comprehensive study analyzes the evolution of market microstructure from 2020-2024, revealing significant changes in trading patterns, liquidity provision, and retail market participation.

## Key Findings

### 1. Bid-Ask Spreads
- **Major finding**: Average spreads decreased 15% across large-cap equities
- **Statistical significance**: p < 0.001 with 95% confidence intervals
- **Contributing factors**: Increased competition, technology improvements

### 2. Trading Volume Changes  
- **Retail participation**: Increased 300% in options trading
- **Market share**: Retail now represents 25% of daily volume (up from 10%)
- **Platform concentration**: Top 3 apps account for 60% of retail flow

### 3. Market Depth Analysis
- **Improved liquidity**: Order book depth increased 40% in major names
- **Fragmentation effects**: Slight decrease in displayed liquidity
- **Dark pool usage**: Institutional flow increasingly off-exchange

## Statistical Analysis Summary
- **Sample size**: 50,000+ daily observations across 500 securities
- **Time period**: January 2020 - December 2024
- **Methods used**: Panel regression, difference-in-differences, event studies
- **Robustness**: Results consistent across multiple model specifications

## Policy Recommendations

### 1. Enhanced Market Surveillance
Recommend expanded monitoring of retail flow concentration and gamification effects.

### 2. PFOF Transparency
Implement enhanced disclosure requirements for payment for order flow arrangements.

### 3. Liquidity Provider Incentives
Consider market maker programs to maintain depth during volatile periods.

## Data Sources and Methodology
- Primary data: SEC EDGAR, FINRA, major exchanges
- Analysis tools: Python (pandas, scipy), R (econometrics)
- Statistical software: Stata for robustness checks

## Limitations and Future Research
- Limited access to proprietary trading data
- Recommend longitudinal study of retail trader outcomes
- Suggest analysis of international market comparisons

---
*Report generated during dry run execution*
*Task ID: {task_id}*
*Generated: {ctx.session.state.get('current_datetime', 'unknown')}*
"""
                with open(report_path, 'w') as f:
                    f.write(report_content)
                print(f"[DRY RUN] ‚úÖ Created: {report_path}")
                ctx.session.state['final_report_artifact'] = report_path
                print(f"[DRY RUN] {self.name} generated final report")
                
        elif "Orchestrator" in self.name:
            # Actually create implementation files during dry run
            import os
            import json
            
            # Get the task-specific outputs directory with proper structure
            task_id = ctx.session.state.get('task_id', 'test_research_session')
            base_dir = f"/home/gaen/agents_gaen/department_of_market_intelligence/outputs/{task_id}"
            planning_dir = f"{base_dir}/planning"
            workspace_dir = f"{base_dir}/workspace"
            os.makedirs(planning_dir, exist_ok=True)
            os.makedirs(f"{workspace_dir}/scripts", exist_ok=True)
            
            # Check if we're replanning due to executor failure
            current_execution_status = ctx.session.state.get('execution_status', '')
            error_type = ctx.session.state.get('error_type', '')
            error_details = ctx.session.state.get('error_details', '')
            suggested_fix = ctx.session.state.get('suggested_fix', '')
            
            if current_execution_status == 'critical_error' and error_type:
                print(f"[DRY RUN] {self.name} REPLANNING with error context:")
                print(f"[DRY RUN]   - Error Type: {error_type}")
                print(f"[DRY RUN]   - Error Details: {error_details}")
                print(f"[DRY RUN]   - Suggested Fix: {suggested_fix}")
                print(f"[DRY RUN] {self.name} would UPDATE implementation plan with fixes")
                # Reset execution status since we're addressing the error
                ctx.session.state['execution_status'] = 'pending'
            else:
                print(f"[DRY RUN] {self.name} creating implementation plan")
            
            # Create implementation manifest in planning directory
            manifest_path = f"{planning_dir}/implementation_manifest.json"
            manifest_data = {
                "task_id": task_id,
                "implementation_plan": {
                    "phase": "data_collection_and_analysis",
                    "parallel_tasks": [
                        {
                            "task_id": "data_collection",
                            "description": "Collect market data from multiple sources",
                            "estimated_runtime": "4 hours",
                            "dependencies": [],
                            "outputs": ["raw_data.csv", "data_summary.json"]
                        },
                        {
                            "task_id": "statistical_analysis", 
                            "description": "Perform statistical tests on collected data",
                            "estimated_runtime": "6 hours",
                            "dependencies": ["data_collection"],
                            "outputs": ["statistical_results.json", "significance_tests.csv"]
                        },
                        {
                            "task_id": "visualization",
                            "description": "Generate charts and graphs for key findings",
                            "estimated_runtime": "3 hours", 
                            "dependencies": ["statistical_analysis"],
                            "outputs": ["charts/", "interactive_dashboard.html"]
                        }
                    ],
                    "data_sources": [
                        "yahoo_finance",
                        "fred_economic_data", 
                        "sec_edgar"
                    ],
                    "required_packages": [
                        "pandas>=1.5.0",
                        "numpy>=1.24.0",
                        "scipy>=1.10.0",
                        "matplotlib>=3.6.0",
                        "yfinance>=0.2.0",
                        "fredapi>=0.5.0"
                    ]
                },
                "validation": {
                    "dry_run": True,
                    "created_at": ctx.session.state.get('current_datetime', 'unknown'),
                    "replanning": current_execution_status == 'critical_error'
                }
            }
            
            with open(manifest_path, 'w') as f:
                json.dump(manifest_data, f, indent=2)
            print(f"[DRY RUN] ‚úÖ Created: {manifest_path}")
            
            # Create results extraction script in workspace
            script_path = f"{workspace_dir}/scripts/results_extraction.py"
            script_content = f'''#!/usr/bin/env python3
"""
Results extraction script for {task_id}
Generated during dry run execution
"""

import pandas as pd
import numpy as np
import json
from pathlib import Path

def extract_key_metrics():
    """Extract key metrics from analysis results."""
    
    # Load statistical results
    with open('statistical_results.json', 'r') as f:
        stats = json.load(f)
    
    # Load significance tests
    sig_tests = pd.read_csv('significance_tests.csv')
    
    # Extract key findings
    key_metrics = {{
        "bid_ask_spreads": {{
            "average_decrease": "15%",
            "statistical_significance": "p < 0.001",
            "confidence_interval": "95%"
        }},
        "trading_volume": {{
            "retail_increase": "300%",
            "market_share_change": "10% to 25%",
            "platform_concentration": "60%"
        }},
        "market_depth": {{
            "liquidity_improvement": "40%",
            "fragmentation_effect": "slight decrease",
            "dark_pool_usage": "increasing"
        }}
    }}
    
    return key_metrics

def generate_summary_report():
    """Generate executive summary of findings."""
    
    metrics = extract_key_metrics()
    
    summary = {{
        "task_id": "{task_id}",
        "execution_date": "{ctx.session.state.get('current_datetime', 'unknown')}",
        "key_findings": metrics,
        "sample_size": "50,000+ observations",
        "time_period": "2020-2024",
        "statistical_methods": [
            "panel_regression",
            "difference_in_differences", 
            "event_studies"
        ],
        "policy_recommendations": [
            "Enhanced market surveillance",
            "PFOF transparency requirements",
            "Liquidity provider incentives"
        ],
        "dry_run": True
    }}
    
    return summary

if __name__ == "__main__":
    print("Extracting results for {task_id}...")
    
    # Extract metrics
    metrics = extract_key_metrics()
    print("Key metrics extracted successfully")
    
    # Generate summary
    summary = generate_summary_report()
    print("Summary report generated")
    
    # Save final results
    with open('final_results.json', 'w') as f:
        json.dump(summary, f, indent=2)
    
    print("Results extraction completed!")
'''
            
            with open(script_path, 'w') as f:
                f.write(script_content)
            print(f"[DRY RUN] ‚úÖ Created: {script_path}")
            
            ctx.session.state['implementation_manifest_artifact'] = manifest_path
            ctx.session.state['results_extraction_script_artifact'] = script_path
            
        elif "Coder" in self.name:
            # Show task-specific information for parallel coding
            coder_subtask = ctx.session.state.get('coder_subtask', None)
            if coder_subtask:
                # Handle both dict and TaskInfo model
                if hasattr(coder_subtask, 'task_id'):
                    # It's a TaskInfo model
                    task_id = coder_subtask.task_id
                    description = coder_subtask.description
                    dependencies = coder_subtask.dependencies
                else:
                    # It's a dict (legacy format)
                    task_id = coder_subtask.get('task_id', 'unknown')
                    description = coder_subtask.get('description', 'no description')
                    dependencies = coder_subtask.get('dependencies', [])
                    
                print(f"[DRY RUN] {self.name} would generate code for:")
                print(f"[DRY RUN]   - Task: {task_id}")
                print(f"[DRY RUN]   - Description: {description}")
                print(f"[DRY RUN]   - Dependencies: {dependencies}")
            else:
                print(f"[DRY RUN] {self.name} would generate code")
            
        elif "Executor" in self.name:
            # Actually create execution results during dry run
            import os
            import json
            
            # Get the task-specific outputs directory with proper structure
            task_id = ctx.session.state.get('task_id', 'test_research_session')
            base_dir = f"/home/gaen/agents_gaen/department_of_market_intelligence/outputs/{task_id}"
            results_dir = f"{base_dir}/results"
            workspace_dir = f"{base_dir}/workspace"
            os.makedirs(results_dir, exist_ok=True)
            os.makedirs(f"{workspace_dir}/src", exist_ok=True)
            
            print(f"[DRY RUN] {self.name} executing experiments and creating result files")
            
            # Create execution results in results directory
            results_path = f"{results_dir}/execution_results.json"
            results_data = {
                "task_id": task_id,
                "execution_summary": {
                    "status": "completed",
                    "start_time": ctx.session.state.get('current_datetime', 'unknown'),
                    "experiments_completed": 4,
                    "total_runtime": "13 hours",
                    "data_points_analyzed": 52000
                },
                "experiment_results": {
                    "bid_ask_spread_analysis": {
                        "status": "success",
                        "key_finding": "15% decrease in average spreads",
                        "statistical_significance": "p < 0.001",
                        "sample_size": 15000,
                        "confidence_interval": "95%"
                    },
                    "market_depth_regression": {
                        "status": "success", 
                        "key_finding": "40% improvement in order book depth",
                        "r_squared": 0.76,
                        "significant_factors": ["retail_volume", "volatility", "time_trend"]
                    },
                    "trading_volume_comparison": {
                        "status": "success",
                        "retail_increase": "300%",
                        "institutional_change": "15%",
                        "market_share_shift": "10% to 25%"
                    },
                    "regulatory_impact_assessment": {
                        "status": "success",
                        "events_analyzed": 12,
                        "significant_impacts": 8,
                        "average_effect_size": "medium"
                    }
                },
                "quality_metrics": {
                    "data_completeness": "98.5%",
                    "outliers_detected": 247,
                    "missing_data_imputed": "1.2%",
                    "robustness_checks_passed": "all"
                },
                "outputs_created": [
                    "statistical_results.json",
                    "significance_tests.csv", 
                    "charts/bid_ask_trends.png",
                    "charts/volume_analysis.png",
                    "charts/regulatory_events.png",
                    "interactive_dashboard.html"
                ],
                "dry_run_metadata": {
                    "simulated": True,
                    "actual_execution": False,
                    "validation_passed": True
                }
            }
            
            with open(results_path, 'w') as f:
                json.dump(results_data, f, indent=2)
            print(f"[DRY RUN] ‚úÖ Created: {results_path}")
            
            # Create mock statistical results in results directory
            stats_path = f"{results_dir}/statistical_results.json"
            stats_data = {
                "task_id": task_id,
                "statistical_tests": {
                    "t_tests": {
                        "bid_ask_spreads_2020_vs_2024": {
                            "t_statistic": -12.45,
                            "p_value": 0.0001,
                            "effect_size": 0.68,
                            "interpretation": "highly_significant_decrease"
                        }
                    },
                    "regression_analysis": {
                        "market_depth_model": {
                            "r_squared": 0.76,
                            "f_statistic": 234.5,
                            "p_value": 0.0000,
                            "coefficients": {
                                "retail_volume": 0.42,
                                "volatility": -0.18,
                                "time_trend": 0.23
                            }
                        }
                    },
                    "event_studies": {
                        "regulatory_announcements": {
                            "abnormal_returns": {
                                "day_minus_1": 0.002,
                                "day_0": -0.008,
                                "day_plus_1": 0.001
                            },
                            "cumulative_abnormal_return": -0.005,
                            "statistical_significance": "p < 0.05"
                        }
                    }
                },
                "dry_run": True
            }
            
            with open(stats_path, 'w') as f:
                json.dump(stats_data, f, indent=2)
            print(f"[DRY RUN] ‚úÖ Created: {stats_path}")
            
            # Create final results summary in results directory
            final_results_path = f"{results_dir}/final_results.json"
            final_data = {
                "task_id": task_id,
                "execution_date": ctx.session.state.get('current_datetime', 'unknown'),
                "key_findings": {
                    "bid_ask_spreads": {
                        "average_decrease": "15%",
                        "statistical_significance": "p < 0.001",
                        "confidence_interval": "95%"
                    },
                    "trading_volume": {
                        "retail_increase": "300%", 
                        "market_share_change": "10% to 25%",
                        "platform_concentration": "60%"
                    },
                    "market_depth": {
                        "liquidity_improvement": "40%",
                        "fragmentation_effect": "slight decrease",
                        "dark_pool_usage": "increasing"
                    }
                },
                "sample_size": "50,000+ observations",
                "time_period": "2020-2024",
                "statistical_methods": [
                    "panel_regression",
                    "difference_in_differences",
                    "event_studies"
                ],
                "policy_recommendations": [
                    "Enhanced market surveillance",
                    "PFOF transparency requirements", 
                    "Liquidity provider incentives"
                ],
                "dry_run": True
            }
            
            with open(final_results_path, 'w') as f:
                json.dump(final_data, f, indent=2)
            print(f"[DRY RUN] ‚úÖ Created: {final_results_path}")
            
            # Create example workspace files for coders
            data_collection_script = f"{workspace_dir}/src/data_collection.py"
            with open(data_collection_script, 'w') as f:
                f.write(f'''#!/usr/bin/env python3
"""
Data collection module for {task_id}
Generated during dry run - coders work here
"""

import pandas as pd
import yfinance as yf
from datetime import datetime, timedelta

def collect_market_data():
    """Collect market microstructure data from multiple sources."""
    # TODO: Implement actual data collection
    pass

def collect_bid_ask_spreads():
    """Collect bid-ask spread data for analysis.""" 
    # TODO: Implement spread data collection
    pass

if __name__ == "__main__":
    print("Starting data collection for {task_id}...")
    # Entry point for data collection scripts
''')
            print(f"[DRY RUN] ‚úÖ Created: {data_collection_script}")
            
            analysis_script = f"{workspace_dir}/src/statistical_analysis.py" 
            with open(analysis_script, 'w') as f:
                f.write(f'''#!/usr/bin/env python3
"""
Statistical analysis module for {task_id}
Generated during dry run - coders work here
"""

import pandas as pd
import numpy as np
from scipy import stats

def analyze_bid_ask_spreads(data):
    """Analyze bid-ask spread evolution."""
    # TODO: Implement spread analysis
    pass

def market_depth_regression(data):
    """Perform regression analysis on market depth."""
    # TODO: Implement regression model
    pass

def statistical_significance_tests(data):
    """Run statistical significance tests."""
    # TODO: Implement t-tests and other statistical tests
    pass
''')
            print(f"[DRY RUN] ‚úÖ Created: {analysis_script}")
            
            # Create a README for the workspace
            readme_path = f"{workspace_dir}/README.md"
            with open(readme_path, 'w') as f:
                f.write(f'''# Workspace for {task_id}

## Directory Structure

- `src/` - Main source code for data collection and analysis
- `notebooks/` - Jupyter notebooks for exploratory analysis
- `scripts/` - Utility scripts and automation
- `tests/` - Unit tests for the codebase

## Getting Started

1. Install dependencies: `pip install -r requirements.txt`
2. Run data collection: `python src/data_collection.py`
3. Run analysis: `python src/statistical_analysis.py`

## Development Guidelines

- All analysis code goes in `src/`
- Use notebooks for exploration and visualization
- Add tests for critical analysis functions
- Document your findings in the results directory
''')
            print(f"[DRY RUN] ‚úÖ Created: {readme_path}")
            
            ctx.session.state['execution_status'] = 'success'  # Mock success
            ctx.session.state['final_results_artifact'] = final_results_path
            
        # Brief processing simulation (no delay)
        
        # Yield a simple completion event
        yield Event(
            author=self.name,
            actions=EventActions()
        )

def create_mock_llm_agent(name: str, instruction: str = "", **kwargs):
    """Factory function to create mock LLM agents."""
    return MockLlmAgent(name=name, instruction=instruction, **kwargs)